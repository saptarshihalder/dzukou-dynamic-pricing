name: scrape
on:
  schedule:
    - cron: '0 */6 * * *'  # Run every 6 hours
  workflow_dispatch:  # Allow manual triggering

jobs:
  run:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install Chrome and dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y wget unzip fonts-liberation libasound2 libatk-bridge2.0-0 libatk1.0-0 libatspi2.0-0 libcups2 libdbus-1-3 libdrm2 libgbm1 libgtk-3-0 libnspr4 libnss3 libxcomposite1 libxdamage1 libxfixes3 libxkbcommon0 libxrandr2 xdg-utils
        wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
        sudo apt install -y ./google-chrome-stable_current_amd64.deb
        google-chrome --version
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install selenium webdriver-manager
    
    - name: Create data directory
      run: mkdir -p data/raw
    
    - name: Run Selenium scraper
      run: |
        python selenium_scraper.py
      env:
        # Tell webdriver-manager to use the system Chrome
        WDM_LOCAL: 'true'
        # Avoid Chrome crashing in GitHub Actions
        PYTHONUNBUFFERED: '1'
    
    - name: Commit changes
      uses: EndBug/add-and-commit@v9
      with:
        add: 'data/raw/*'
        message: 'auto: update price data'
        committer_name: 'GitHub Actions'
        committer_email: 'actions@github.com' 