name: Price Scraper

on:
  workflow_dispatch:
    inputs:
      scraper_type:
        description: 'Scraper to use (simple or master)'
        required: true
        default: 'simple'
        type: choice
        options:
          - simple
          - master
      master_csv_path:
        description: 'Path to master CSV file (for master scraper)'
        required: false
        default: 'data/master/product_master_clean.csv'
  schedule:
    - cron: '0 */6 * * *'  # Run every 6 hours
  push:
    branches: [ main ]

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
      with:
        lfs: true
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install Chrome and dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y wget unzip xvfb libxi6 libgconf-2-4 libxss1 libappindicator1 libindicator7 libnss3 libgbm1
        wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
        sudo dpkg -i google-chrome-stable_current_amd64.deb || sudo apt-get -f install -y
        google-chrome --version
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Create data directories
      run: |
        mkdir -p data/raw data/processed logs

    - name: Determine scraper to use
      id: scraper
      run: |
        if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
          echo "type=${{ github.event.inputs.scraper_type }}" >> $GITHUB_OUTPUT
          echo "master_csv=${{ github.event.inputs.master_csv_path }}" >> $GITHUB_OUTPUT
        else
          echo "type=simple" >> $GITHUB_OUTPUT
          echo "master_csv=data/master/product_master_clean.csv" >> $GITHUB_OUTPUT
        fi
    
    - name: Run CSV-based scraper
      if: steps.scraper.outputs.type == 'simple'
      run: |
        python src/scrapers/csv_scraper.py products.csv --headless --debug
    
    - name: Run Master Product scraper
      if: steps.scraper.outputs.type == 'master'
      run: |
        python src/scrapers/master_csv_scraper.py ${{ steps.scraper.outputs.master_csv }} --headless --debug
      
    - name: Setup Git LFS
      run: |
        git lfs install
        
    - name: Configure Git
      run: |
        git config --global user.name "GitHub Actions Bot"
        git config --global user.email "actions@github.com"
        
    - name: Commit and push scraped data
      run: |
        # Add any new files that were created
        git add data/raw/*.json
        git add data/processed/*.csv
        git add logs/*.txt
        git add debug/*.html || true
        
        # Check if there are changes to commit
        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          git commit -m "Update scraped data [automated]"
          git push
        fi 